{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "```\n",
    "Author:\n",
    "Zach Wolpe\n",
    "zachcolinwolpe@gmail.com\n",
    "www.zachwolpe.com\n",
    "```\n",
    "\n",
    "\n",
    "A simple but efficient data preprocessing template.\n",
    "\n",
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing Template\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "print(dataset)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "It is careless to simple remove missing data. \n",
    "\n",
    "## Continuous variables\n",
    "One popular approach for handling missing data is to simple insert the mean of the dataset into the missing values. \n",
    "\n",
    "The two missing values have be replaced with the column means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking care of missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variables\n",
    "\n",
    "One-hot encoding (creating dummy variables) can be readily done with sklearn. \n",
    "\n",
    "Remember encoding one less variable than actual categories is sufficient in capturing all infomation. This code simply encodes (gives a numeric value to) categorical variables & creates 1 dummy variable (0 or 1) for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.40000000e+01,\n",
       "         7.20000000e+04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 2.70000000e+01,\n",
       "         4.80000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.00000000e+01,\n",
       "         5.40000000e+04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.80000000e+01,\n",
       "         6.10000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.00000000e+01,\n",
       "         6.37777778e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.50000000e+01,\n",
       "         5.80000000e+04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.87777778e+01,\n",
       "         5.20000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.80000000e+01,\n",
       "         7.90000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 5.00000000e+01,\n",
       "         8.30000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.70000000e+01,\n",
       "         6.70000000e+04]]), array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Numeric Encoding \n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "\n",
    "# Create Dummy Variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test-Split\n",
    "\n",
    "Perform splits AFTER preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "It is often advicable to scale features, to prevent the model from overweighting variables of a larger magnitude.\n",
    "\n",
    "This over-weighting often occurs as a result of distance calculation. Many machine learning models are based on Euclidean distance & as such distances of different scales may hinder model performance. (example, square distance from the mean).\n",
    "\n",
    "Feature scaling also ensures the algorithm converges much faster.\n",
    "\n",
    "It may not be neccessary to scale dummy variables as they are $\\in [0,1]$ bound. This is often a matter of preference.\n",
    "\n",
    "Two popular approaches to feature scaling:\n",
    "\n",
    "#### Standardization \n",
    "$$X_{std} = \\frac{X - \\bar{X}}{std(X)}$$\n",
    "\n",
    "#### Normalization \n",
    "$$X_{norm} = \\frac{X - min(X)}{max(X) - min(X)}$$\n",
    "\n",
    "\n",
    "###### Note: \n",
    "The Y variable is not scaled in the template because it is binary (discrete). Feature scaling is required for continuous dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77459667, -0.77459667,  1.73205081, -0.25987589, -0.35492486],\n",
       "       [-0.77459667,  1.29099445, -0.57735027,  0.06553392, -0.09005556],\n",
       "       [ 1.29099445, -0.77459667, -0.57735027, -0.74799061, -0.6409837 ],\n",
       "       [ 1.29099445, -0.77459667, -0.57735027,  1.36717316,  1.3614282 ],\n",
       "       [ 1.29099445, -0.77459667, -0.57735027, -0.4225808 ,  0.21719283],\n",
       "       [-0.77459667,  1.29099445, -0.57735027,  1.69258297,  1.74283999],\n",
       "       [-0.77459667,  1.29099445, -0.57735027, -1.56151513, -1.0223955 ],\n",
       "       [-0.77459667, -0.77459667,  1.73205081, -0.13332763, -1.21310139]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "#sc_y = StandardScaler()\n",
    "#y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
